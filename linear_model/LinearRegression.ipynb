{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=load_boston()\n",
    "X=data.data\n",
    "Y=data.target\n",
    "\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据$X$是一个$(n{\\times}m)$的矩阵，每一行是一个样本，每一列代表一个特征："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=X_train.shape[0]\n",
    "m=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标签$Y$是一个列向量，其行数与$X$相同："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = Y_train.reshape((n, 1))\n",
    "Y_test = Y_test.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 粗略模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型表达式为：\n",
    "$$\n",
    "\\hat{Y}=XW^{T}+b\n",
    "$$\n",
    "其中权重系数$W$的形状为$(1,m)$，偏置系数$b$为单变量系数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.rand(m).reshape((1, -1))  # 权重，行向量\n",
    "b = np.ones((1, 1))  # 偏置\n",
    "\n",
    "Y_hat=np.dot(X_train, W.T)+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型的损失函数为：\n",
    "$$\n",
    "\\begin{align}\n",
    "L&=\\sum\\limits_{i=1}^n(y^{(i)}-\\hat{y}^{(i)})^{2} \\\\\n",
    "&=\\frac{1}{n}(Y-\\hat{Y})^{T}(Y-\\hat{Y}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "损失函数关于参数$W$与$b$的梯度可以求得：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial{L}}{\\partial{W}}&=\\frac{2}{n}(\\hat{Y}-Y)^{T}{\\cdot}X \\\\\n",
    "\\frac{\\partial{L}}{\\partial{b}}&=\\frac{2}{n}(\\hat{Y}-Y)^{T}{\\cdot}[1,1,...,1]^{T} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW = 2 * (Y_hat - Y_train).T.dot(X_train) / n\n",
    "db = 2 * (Y_hat - Y_train).T.dot(np.ones((n, 1))) / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数的迭代更新公式：\n",
    "$$\n",
    "W:=W-{\\alpha}\\frac{\\partial{L}}{\\partial{W}}, \\quad b:b-{\\alpha}\\frac{\\partial{L}}{\\partial{b}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter=2000\n",
    "alpha=0.000003        # 注意学习率过大会导致震荡，然后误差越来越大\n",
    "\n",
    "for i in range(max_iter+1):\n",
    "    Y_hat=np.dot(X_train, W.T)+b\n",
    "    \n",
    "    dW = 2 * (Y_hat - Y_train).T.dot(X_train) / n\n",
    "    db = 2 * (Y_hat - Y_train).T.dot(np.ones((n, 1))) / n\n",
    "    \n",
    "    W = W - alpha * dW\n",
    "    b = b - alpha * db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用该模型分别对训练集与预测集做预测："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_train=np.dot(X_train, W.T) + b\n",
    "Y_pred_test=np.dot(X_test, W.T) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个RMSE损失函数来评价模型在测试集上的表现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[185.09627015] [91.3613369]\n"
     ]
    }
   ],
   "source": [
    "def RMSE(Y_true,Y_pred):\n",
    "    return sum((Y_true-Y_pred)**2)**0.5\n",
    "\n",
    "print(RMSE(Y_train,Y_pred_train),RMSE(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型简单打包："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7900.53323951]\t[319.24007824]\t[270.49955989]\t[243.49382325]\t[226.93698864]\t[215.69702349]\t[207.26728937]\t[200.41234936]\t[194.52248825]\t[189.29220703]\t[184.56232131]\t"
     ]
    }
   ],
   "source": [
    "def linear_reg(X,Y,alpha=0.000003,max_iter=2000):\n",
    "    n=X.shape[0]\n",
    "    m=X.shape[1]\n",
    "    \n",
    "    W = np.random.rand(m).reshape((1, -1))  # 权重，行向量\n",
    "    b = np.ones((1, 1))  # 偏置\n",
    "\n",
    "    for i in range(max_iter+1):\n",
    "        Y_hat=np.dot(X, W.T)+b\n",
    "\n",
    "        dW = 2 * (Y_hat - Y).T.dot(X) / n\n",
    "        db = 2 * (Y_hat - Y).T.dot(np.ones((n, 1))) / n\n",
    "\n",
    "        W = W - alpha * dW\n",
    "        b = b - alpha * db\n",
    "        \n",
    "        if i%200==0:\n",
    "            Y_hat=np.dot(X, W.T)+b\n",
    "            L=sum((Y-Y_hat)**2)**0.5\n",
    "            print(L,end='\\t')\n",
    "        \n",
    "    return W,b\n",
    "\n",
    "W,b=linear_reg(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据归一化\n",
    "**Normalization：**\n",
    "$$\n",
    "x=\\frac{x-x_{min}}{x_{max}-x_{min}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.row_stack((X_train,X_test))\n",
    "\n",
    "X_max=X.max(axis=0)\n",
    "X_min=X.min(axis=0)\n",
    "\n",
    "X_train_norm=(X_train-X_min)/(X_max-X_min)\n",
    "X_test_norm=(X_test-X_min)/(X_max-X_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对数据归一化之后再测试模型表现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[424.15964286]\t[422.76335186]\t[421.37487952]\t[419.99419075]\t[418.62125058]\t[417.25602415]\t[415.89847673]\t[414.54857372]\t[413.20628062]\t[411.87156307]\t[410.5443868]\t"
     ]
    }
   ],
   "source": [
    "W,b=linear_reg(X_train_norm,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为数据做了归一化，整个数据集上的梯度分布得到了改良，所以可以调大学习率，由此可以看出数据标准化在线性回归上的威力："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[230.97689553]\t[101.66111864]\t[95.44056473]\t[93.82026292]\t[93.06961102]\t[92.62523337]\t[92.339228]\t[92.14829982]\t[92.01792709]\t[91.92740293]\t[91.86372418]\t"
     ]
    }
   ],
   "source": [
    "W,b=linear_reg(X_train_norm,Y_train,alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardization：**\n",
    "$$\n",
    "x=\\frac{x-x_{\\mu}}{\\sigma}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.row_stack((X_train,X_test))\n",
    "\n",
    "X_avg=X.mean(axis=0)\n",
    "X_std=X.std(axis=0)\n",
    "\n",
    "X_train_std=(X_train-X_avg)/X_std\n",
    "X_test_std=(X_test-X_avg)/X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[477.3276253]\t[476.49028898]\t[475.65760953]\t[474.82953786]\t[474.00602539]\t[473.18702411]\t[472.37248652]\t[471.56236566]\t[470.75661508]\t[469.95518885]\t[469.15804155]\t"
     ]
    }
   ],
   "source": [
    "W,b=linear_reg(X_train_std,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[368.73447224]\t[91.70661095]\t[91.69985793]\t[91.69982672]\t[91.69982658]\t[91.69982658]\t[91.69982658]\t[91.69982658]\t[91.69982658]\t[91.69982658]\t[91.69982658]\t"
     ]
    }
   ],
   "source": [
    "W,b=linear_reg(X_train_std,Y_train,alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个简单实验比较发现经Standardization后的数据更加有助于模型的收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mini-batch梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LinearRegression:\n",
    "#     def __init__(self, lr=0.00001, batch_size=32, max_iter=1000):\n",
    "#         self.lr = lr\n",
    "#         self.batch_size = batch_size\n",
    "#         self.max_iter = max_iter\n",
    "#         self.W = None\n",
    "#         self.b = None\n",
    "\n",
    "#     def fit(self, X, Y):\n",
    "#         X = X.copy()\n",
    "#         Y = Y.copy()\n",
    "\n",
    "#         n = X.shape[0]  # 样本数\n",
    "#         m = X.shape[1]  # 特征数\n",
    "#         assert Y.shape[0] == n  # 数据与标签应该相等\n",
    "#         Y = Y.reshape((n, 1))  # 标签，列向量\n",
    "\n",
    "#         self.W = np.random.rand(m).reshape((1, -1))  # 权重，行向量\n",
    "#         self.b = np.ones((1, 1))  # 偏置\n",
    "\n",
    "#         assert Y.shape == (n, 1)\n",
    "\n",
    "#         num_batch = n // self.batch_size\n",
    "\n",
    "#         for epoch in range(self.max_iter):\n",
    "#             for i in range(num_batch + 1):\n",
    "#                 start_index = i * self.batch_size\n",
    "#                 end_index = (i + 1) * self.batch_size\n",
    "#                 if end_index <= n:\n",
    "#                     X_batch = X[start_index:end_index + 1]\n",
    "#                     Y_batch = Y[start_index:end_index + 1]\n",
    "#                 else:\n",
    "#                     X_batch = X[start_index:]\n",
    "#                     Y_batch = Y[start_index:]\n",
    "\n",
    "#                 Y_hat = X_batch.dot(self.W.T) + self.b\n",
    "#                 dW = 2 * (Y_hat - Y_batch).T.dot(X_batch) / n\n",
    "#                 db = 2 * (Y_hat - Y_batch).T.dot(np.ones((X_batch.shape[0], 1))) / n\n",
    "#                 assert (dW.shape == self.W.shape) & (db.shape == self.b.shape)\n",
    "\n",
    "#                 self.W = self.W - self.lr * dW\n",
    "#                 self.b = self.b - self.lr * db\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         X = X.copy()\n",
    "#         return np.squeeze(np.dot(X, self.W.T) + self.b)        # 将矩阵压缩成向量，与原始输入Y保持一致\n",
    "\n",
    "    \n",
    "# line_reg=LinearRegression()\n",
    "# line_reg.fit(X_train,Y_train)\n",
    "\n",
    "# def RMSE(Y_true,Y_pred):\n",
    "#     return sum((Y_true-Y_pred)**2)**0.5\n",
    "\n",
    "# Y_pred=line_reg.predict(X_test)\n",
    "# RMSE(Y_test,Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
