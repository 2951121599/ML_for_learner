{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文件是linear regression在实现方面的细节，具体理论参见我的博客。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=load_boston()\n",
    "X=data.data\n",
    "Y=data.target\n",
    "\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据$X$是一个$(n{\\times}m)$的矩阵，每一行是一个样本，每一列代表一个特征："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=X_train.shape[0]\n",
    "m=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标签$Y$是一个列向量，其行数与$X$相同："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = Y_train.reshape((n, 1))\n",
    "Y_test = Y_test.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 粗略模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型表达式为：\n",
    "$$\n",
    "\\hat{Y}=XW^{T}+b\n",
    "$$\n",
    "其中权重系数$W$的形状为$(1,m)$，偏置系数$b$为单变量系数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.rand(m).reshape((1, -1))  # 权重，行向量\n",
    "b = np.ones((1, 1))  # 偏置\n",
    "\n",
    "Y_hat=np.dot(X_train, W.T)+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型的损失函数为：\n",
    "$$\n",
    "\\begin{align}\n",
    "L&=\\sum\\limits_{i=1}^n(y^{(i)}-\\hat{y}^{(i)})^{2} \\\\\n",
    "&=\\frac{1}{n}(Y-\\hat{Y})^{T}(Y-\\hat{Y}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "损失函数关于参数$W$与$b$的梯度可以求得：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial{L}}{\\partial{W}}&=\\frac{2}{n}(\\hat{Y}-Y)^{T}{\\cdot}X \\\\\n",
    "\\frac{\\partial{L}}{\\partial{b}}&=\\frac{2}{n}(\\hat{Y}-Y)^{T}{\\cdot}[1,1,...,1]^{T} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW = 2 * (Y_hat - Y_train).T.dot(X_train) / n\n",
    "db = 2 * (Y_hat - Y_train).T.dot(np.ones((n, 1))) / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数的迭代更新公式：\n",
    "$$\n",
    "W:=W-{\\alpha}\\frac{\\partial{L}}{\\partial{W}}, \\quad b:b-{\\alpha}\\frac{\\partial{L}}{\\partial{b}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter=2000\n",
    "alpha=0.000001        # 注意学习率过大会导致震荡，然后误差越来越大\n",
    "\n",
    "for i in range(max_iter+1):\n",
    "    Y_hat=np.dot(X_train, W.T)+b\n",
    "    \n",
    "    dW = 2 * (Y_hat - Y_train).T.dot(X_train) / n\n",
    "    db = 2 * (Y_hat - Y_train).T.dot(np.ones((n, 1))) / n\n",
    "    \n",
    "    W = W - alpha * dW\n",
    "    b = b - alpha * db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用该模型分别对训练集与预测集做预测："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_train=np.dot(X_train, W.T) + b\n",
    "Y_pred_test=np.dot(X_test, W.T) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个RMSE损失函数来评价模型的表现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[209.03039228] [105.01153698]\n"
     ]
    }
   ],
   "source": [
    "def RMSE(Y_true,Y_pred):\n",
    "    return sum((Y_true-Y_pred)**2)**0.5\n",
    "\n",
    "print(RMSE(Y_train,Y_pred_train),RMSE(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型简单打包："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3096.84571667]\t[219.1522414]\t[211.36205226]\t[205.07451308]\t[199.96630972]\t[195.78432519]\t[192.32949228]\t[189.44514806]\t[187.00819393]\t[184.92215757]\t[183.11165466]\t"
     ]
    }
   ],
   "source": [
    "def linear_reg(X,Y,alpha=0.000001,max_iter=2000):\n",
    "    n=X.shape[0]\n",
    "    m=X.shape[1]\n",
    "    \n",
    "    W = np.random.rand(m).reshape((1, -1))  # 权重，行向量\n",
    "    b = np.ones((1, 1))  # 偏置\n",
    "\n",
    "    for i in range(max_iter+1):\n",
    "        Y_hat=np.dot(X, W.T)+b\n",
    "\n",
    "        dW = 2 * (Y_hat - Y).T.dot(X) / n\n",
    "        db = 2 * (Y_hat - Y).T.dot(np.ones((n, 1))) / n\n",
    "\n",
    "        W = W - alpha * dW\n",
    "        b = b - alpha * db\n",
    "        \n",
    "        if i%200==0:\n",
    "            Y_hat=np.dot(X, W.T)+b\n",
    "            L=sum((Y-Y_hat)**2)**0.5\n",
    "            print(L,end='\\t')\n",
    "\n",
    "    return W,b\n",
    "\n",
    "W,b=linear_reg(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据归一化\n",
    "**Normalization：**\n",
    "$$\n",
    "x=\\frac{x-x_{min}}{x_{max}-x_{min}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.row_stack((X_train,X_test))\n",
    "\n",
    "X_max=X.max(axis=0)\n",
    "X_min=X.min(axis=0)\n",
    "\n",
    "X_train_norm=(X_train-X_min)/(X_max-X_min)\n",
    "X_test_norm=(X_test-X_min)/(X_max-X_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对数据归一化之后再测试模型表现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[417.92167284]\t[417.46312635]\t[417.00543837]\t[416.54860759]\t[416.09263273]\t[415.63751251]\t[415.18324563]\t[414.72983082]\t[414.27726679]\t[413.82555226]\t[413.37468596]\t"
     ]
    }
   ],
   "source": [
    "W,b=linear_reg(X_train_norm,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为数据做了归一化，整个数据集上的梯度分布得到了改良，所以可以调大学习率，由此可以看出数据标准化在线性回归上的威力："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[229.5317887]\t[102.93234471]\t[97.0599989]\t[95.47338681]\t[94.67825928]\t[94.1663453]\t[93.81118957]\t[93.55803605]\t[93.37505064]\t[93.24157706]\t[93.14358568]\t"
     ]
    }
   ],
   "source": [
    "W,b=linear_reg(X_train_norm,Y_train,alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardization：**\n",
    "$$\n",
    "x=\\frac{x-x_{\\mu}}{\\sigma}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.row_stack((X_train,X_test))\n",
    "\n",
    "X_avg=X.mean(axis=0)\n",
    "X_std=X.std(axis=0)\n",
    "\n",
    "X_train_std=(X_train-X_avg)/X_std\n",
    "X_test_std=(X_test-X_avg)/X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[476.48493421]\t[476.19868728]\t[475.91303235]\t[475.62796719]\t[475.34348955]\t[475.05959722]\t[474.77628798]\t[474.49355962]\t[474.21140994]\t[473.92983674]\t[473.64883785]\t"
     ]
    }
   ],
   "source": [
    "W,b=linear_reg(X_train_std,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[370.32667861]\t[92.8702853]\t[92.86133807]\t[92.86127362]\t[92.86127315]\t[92.86127315]\t[92.86127315]\t[92.86127315]\t[92.86127315]\t[92.86127315]\t[92.86127315]\t"
     ]
    }
   ],
   "source": [
    "W,b=linear_reg(X_train_std,Y_train,alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个简单实验比较发现，相比于Normalization，Standardization能够更快地加速模型的收敛。\n",
    "\n",
    "数据归一化工具简单打包："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Standardization(X_train,X_test):\n",
    "    X=np.row_stack((X_train,X_test))\n",
    "\n",
    "    X_avg=X.mean(axis=0)\n",
    "    X_std=X.std(axis=0)\n",
    "\n",
    "    X_train_std=(X_train-X_avg)/X_std\n",
    "    X_test_std=(X_test-X_avg)/X_std\n",
    "    return X_train_std,X_test_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mini-batch梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4054.64707425]\t[266.96807985]\t[253.24645327]\t[245.19175745]\t[240.23696832]\t[236.93782462]\t[234.51337405]\t[232.55169565]\t[230.83919826]\t[229.26669286]\t[227.77916272]\t"
     ]
    }
   ],
   "source": [
    "def linear_reg(X,Y,alpha=0.000001,max_iter=2000,batch_size=99999):\n",
    "    n=X.shape[0]\n",
    "    m=X.shape[1]\n",
    "    num_batch = n // batch_size\n",
    "    \n",
    "    W = np.random.rand(m).reshape((1, -1))  # 权重，行向量\n",
    "    b = np.ones((1, 1))  # 偏置\n",
    "\n",
    "    for epoch in range(max_iter+1):\n",
    "        \n",
    "        ######  mini-batch  ######\n",
    "        for i in range(num_batch + 1):\n",
    "            start_index = i * batch_size\n",
    "            end_index = (i + 1) * batch_size\n",
    "            if end_index <= n:\n",
    "                X_batch = X[start_index:end_index + 1]\n",
    "                Y_batch = Y[start_index:end_index + 1]\n",
    "            else:\n",
    "                X_batch = X[start_index:]\n",
    "                Y_batch = Y[start_index:]\n",
    "                \n",
    "        n_batch=X_batch.shape[0]\n",
    "        Y_hat_batch=np.dot(X_batch, W.T)+b\n",
    "\n",
    "        dW = 2 * (Y_hat_batch - Y_batch).T.dot(X_batch) / n_batch\n",
    "        db = 2 * (Y_hat_batch - Y_batch).T.dot(np.ones((n_batch, 1))) / n_batch\n",
    "        ######  mini-batch  ######\n",
    "        \n",
    "        W = W - alpha * dW\n",
    "        b = b - alpha * db\n",
    "        \n",
    "        if epoch%200==0:\n",
    "            Y_hat=np.dot(X, W.T)+b\n",
    "            L=sum((Y-Y_hat)**2)**0.5\n",
    "            print(L,end='\\t')\n",
    "\n",
    "    return W,b\n",
    "\n",
    "W,b=linear_reg(X_train,Y_train,batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单纯的mini-batch并没有很明显的提升模型表现，我们再加上Standardization："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[473.20273442]\t[338.15078566]\t[281.41435524]\t[250.28595535]\t[231.39037269]\t[218.79341172]\t[209.82423767]\t[203.15932313]\t[198.05613147]\t[194.05332841]\t[190.84553224]\t"
     ]
    }
   ],
   "source": [
    "X_train_std,X_test_std=Standardization(X_train,X_test)\n",
    "W,b=linear_reg(X_train_std,Y_train,alpha=0.001,batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "额，表现并不是很理，甚至在加大max_iter值后模型还是没有收敛，可能是数据量太小，mini-batch引入的随机性对模型的收敛起了一个反作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LinearRegression:\n",
    "#     def __init__(self, lr=0.00001, batch_size=32, max_iter=1000):\n",
    "#         self.lr = lr\n",
    "#         self.batch_size = batch_size\n",
    "#         self.max_iter = max_iter\n",
    "#         self.W = None\n",
    "#         self.b = None\n",
    "\n",
    "#     def fit(self, X, Y):\n",
    "#         X = X.copy()\n",
    "#         Y = Y.copy()\n",
    "\n",
    "#         n = X.shape[0]  # 样本数\n",
    "#         m = X.shape[1]  # 特征数\n",
    "#         assert Y.shape[0] == n  # 数据与标签应该相等\n",
    "#         Y = Y.reshape((n, 1))  # 标签，列向量\n",
    "\n",
    "#         self.W = np.random.rand(m).reshape((1, -1))  # 权重，行向量\n",
    "#         self.b = np.ones((1, 1))  # 偏置\n",
    "\n",
    "#         assert Y.shape == (n, 1)\n",
    "\n",
    "#         num_batch = n // self.batch_size\n",
    "\n",
    "#         for epoch in range(self.max_iter):\n",
    "#             for i in range(num_batch + 1):\n",
    "#                 start_index = i * self.batch_size\n",
    "#                 end_index = (i + 1) * self.batch_size\n",
    "#                 if end_index <= n:\n",
    "#                     X_batch = X[start_index:end_index + 1]\n",
    "#                     Y_batch = Y[start_index:end_index + 1]\n",
    "#                 else:\n",
    "#                     X_batch = X[start_index:]\n",
    "#                     Y_batch = Y[start_index:]\n",
    "\n",
    "#                 Y_hat = X_batch.dot(self.W.T) + self.b\n",
    "#                 dW = 2 * (Y_hat - Y_batch).T.dot(X_batch) / n\n",
    "#                 db = 2 * (Y_hat - Y_batch).T.dot(np.ones((X_batch.shape[0], 1))) / n\n",
    "#                 assert (dW.shape == self.W.shape) & (db.shape == self.b.shape)\n",
    "\n",
    "#                 self.W = self.W - self.lr * dW\n",
    "#                 self.b = self.b - self.lr * db\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         X = X.copy()\n",
    "#         return np.squeeze(np.dot(X, self.W.T) + self.b)        # 将矩阵压缩成向量，与原始输入Y保持一致\n",
    "\n",
    "    \n",
    "# line_reg=LinearRegression()\n",
    "# line_reg.fit(X_train,Y_train)\n",
    "\n",
    "# def RMSE(Y_true,Y_pred):\n",
    "#     return sum((Y_true-Y_pred)**2)**0.5\n",
    "\n",
    "# Y_pred=line_reg.predict(X_test)\n",
    "# RMSE(Y_test,Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
