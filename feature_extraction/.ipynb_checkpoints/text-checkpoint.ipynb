{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append(os.path.dirname(os.path.abspath('.')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预处理并生成词袋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def preprocess(string):\n",
    "    string = string.lower()    # 小写化\n",
    "    string = re.sub(r\"[^a-zA-Z0-9]\", \" \", string)     # 标点转空格\n",
    "    return string\n",
    "\n",
    "\n",
    "bag = set()    # 词袋\n",
    "for string in corpus:\n",
    "    bag.update(preprocess(string).split())\n",
    "\n",
    "bag = np.array(sorted(list(bag)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def count_word(string):\n",
    "    '''\n",
    "    对单个字串的向量化\n",
    "    '''\n",
    "    string = preprocess(string)\n",
    "    res = [0 for _ in range(len(bag))]\n",
    "    cnts = dict(Counter(string.split()))\n",
    "    for idx in range(len(bag)):\n",
    "        res[idx] = cnts.get(bag[idx], 0)\n",
    "    return res\n",
    "\n",
    "\n",
    "vec = np.array(list(map(count_word, corpus)))\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from text import CountVectorizer\n",
    "from scipy import sparse\n",
    "\n",
    "# tf_arr = CountVectorizer().fit_transform(corpus)    # 各单词在各文档中的tf\n",
    "tf_arr = np.array([[3, 0, 1],\n",
    "                   [2, 0, 0],\n",
    "                   [3, 0, 0],\n",
    "                   [4, 0, 0],\n",
    "                   [3, 2, 0],\n",
    "                   [3, 0, 2]])\n",
    "# tf_arr = sparse.csr_matrix(tf_arr)\n",
    "# n_D = tf_arr.shape[0]\n",
    "# df_vec = (tf_arr != 0).sum(axis=0)    # 各单词的df，(voc_size,)\n",
    "# idf_vec = np.log(n_D/df_vec)+1    # (voc_size,)\n",
    "# raw_tfidf = tf_arr*idf_vec\n",
    "# # raw_tfidf=tf_arr.multiply(sparse.csr_matrix(idf_vec))\n",
    "# tfidf = raw_tfidf/np.sqrt(np.square(raw_tfidf).sum(axis=1)).reshape((-1, 1))\n",
    "# tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.85151335, 0.        , 0.52433293],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.55422893, 0.83236428, 0.        ],\n",
       "       [0.63035731, 0.        , 0.77630514]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = [[3, 0, 1],\n",
    "          [2, 0, 0],\n",
    "          [3, 0, 0],\n",
    "          [4, 0, 0],\n",
    "          [3, 2, 0],\n",
    "          [3, 0, 2]]\n",
    "\n",
    "from text import TfidfTransformer\n",
    "transformer=TfidfTransformer()\n",
    "transformer.fit_transform(counts).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.85151335, 0.        , 0.52433293],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.55422893, 0.83236428, 0.        ],\n",
       "       [0.63035731, 0.        , 0.77630514]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer=TfidfTransformer()\n",
    "transformer.fit_transform(counts).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
